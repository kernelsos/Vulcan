# -*- coding: utf-8 -*-
"""Alpaca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Bsoq1MKLBlWsa1FnRoYpFifvVUnG4kq
"""

!pip install transformers datasets accelerate peft bitsandbytes

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType
from transformers import DataCollatorForLanguageModeling
from transformers import BitsAndBytesConfig

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype='float16'
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

data = load_dataset("json", data_files="newDataset_20k.json")

def tokenize(example):
    messages = (
        f"<|im_start|>system\n{example['system']}<|im_end|>\n"
        f"<|im_start|>user\n{example['user']}<|im_end|>\n"
        f"<|im_start|>assistant\n{example['assistant']}<|im_end|>"
    )
    return tokenizer(messages, truncation=True, padding='max_length', max_length=512)

tokenized_data = data["train"].map(tokenize)

training_args = TrainingArguments(
    output_dir="./tiny-assistant",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    num_train_epochs=1,
    logging_steps=10,
    save_strategy="epoch",
    bf16=False,
    fp16=True,
    report_to="none"
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
prompt = "<|system|>\nYou are a helpful assistant.\n<|user|>\nWhat is Java?\n<|assistant|>\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(output[0], skip_special_tokens=True))

#from google.colab import drive
#drive.mount('/content/drive')

# Copy trained model to your Drive
#!cp -r ./tiny-assistant /content/drive/MyDrive/



# TO ovveride the older version by newer trained version.

from google.colab import drive
drive.mount('/content/drive')

!cp -r ./tiny-assistant /content/drive/MyDrive/

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Mount Google Drive (if needed)
from google.colab import drive
drive.mount('/content/drive')

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# Load LoRA fine-tuned adapter
model = PeftModel.from_pretrained(base_model, "/content/drive/MyDrive/tiny-assistant/checkpoint-5006")

from transformers import pipeline, AutoTokenizer
import torch

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# Create pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Create a prompt in ChatML style
prompt = """<|system|>
You are a helpful assistant.
<|user|>
What is JavaScript and write a simple code in javascript?
<|assistant|>
"""

# Generate output
output = pipe(prompt, max_new_tokens=100, temperature=0.7, do_sample=True)

# Print the assistant's reply
print(output[0]['generated_text'])

#Training ----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Commented out IPython magic to ensure Python compatibility.
# %pip install trl

from datasets import load_dataset

# Load local dataset
data = load_dataset("json", data_files="newDataset.json")

from transformers import TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# Define max length (change if needed)
max_seq_length = 1024

# Optional: packing = True to concatenate short examples (saves memory)
packing = False

# Define output directory
output_dir = "/content/drive/MyDrive/tiny-assistant-magicoder"

# Define training arguments
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=2,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
    max_steps=2000,
    bf16=False,
    fp16=True,
    optim="paged_adamw_8bit",  # bitsandbytes optim
    lr_scheduler_type="cosine",
    warmup_steps=100,
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=data["train"],
    args=training_arguments,
    dataset_text_field="text",  # We will fix this in a moment
    max_seq_length=max_seq_length,
    packing=packing,
)

from peft import PeftModel
from transformers import AutoModelForCausalLM

# Load base model and LoRA adapter
base_model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
model = PeftModel.from_pretrained(base_model, "/content/drive/MyDrive/tiny-assistant/checkpoint-5006")

# Merge LoRA into the base model
model = model.merge_and_unload()

model.save_pretrained("merged_model")
tokenizer.save_pretrained("merged_model")

!zip -r merged_model.zip merged_model/



import json

input_path = "03_code_alpaca_2k_chatml.jsonl"
output_path = "output_chatml_format.jsonl"

def convert_messages_to_chatml(messages):
    chatml = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        chatml += f"<|im_start|>{role}\n{content}<|im_end|>\n"
    return chatml.strip()  # Remove trailing newline

with open(input_path, "r", encoding="utf-8") as infile, open(output_path, "w", encoding="utf-8") as outfile:
    for line in infile:
        data = json.loads(line)
        messages = data.get("messages", [])
        chatml_text = convert_messages_to_chatml(messages)
        new_line = {"text": chatml_text}
        outfile.write(json.dumps(new_line, ensure_ascii=False) + "\n")

print(f"âœ… Converted and saved to {output_path}")





