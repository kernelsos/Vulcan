{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate peft bitsandbytes"
      ],
      "metadata": {
        "id": "DDUv8LK8811f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "4Y2IXaEl81yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import BitsAndBytesConfig #For quantization\n",
        "\n",
        "#Base Model\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype='float16'\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # The higher the value of r allows for more complex updates, potentially leading to better performance on complex tasks :D .\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "#Dataset : In my case i used ChatML style dataset in a jsonl file format. In jsonl format every line conatians a json object.\n",
        "data = load_dataset(\"json\", data_files=\"Final_dataset.json\")\n",
        "\n",
        "#Preparation and tokenization of data\n",
        "def tokenize(example):\n",
        "    messages = (\n",
        "        f\"<|im_start|>instruction\\n{example['instruction']}<|im_end|>\\n\"\n",
        "        f\"<|im_start|>input\\n{example['input']}<|im_end|>\\n\"\n",
        "        f\"<|im_start|>output\\n{example['output']}<|im_end|>\"\n",
        "    )\n",
        "    return tokenizer(messages, truncation=True, padding='max_length', max_length=512)\n",
        "# Mapping of tokens\n",
        "tokenized_data = data[\"train\"].map(tokenize)\n",
        "\n",
        "# If you increase the number of epochs and per_device_train_batch_size and gradient_accumulation_steps , the training time will going to increase\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tiny-assistant\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=False,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "collapsed": true,
        "id": "oN4VIrc681vt",
        "outputId": "9649a546-5438-4050-bd8e-72032bc892c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1586301333.py, line 68)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1586301333.py\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    add co\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive and Save the trained weights on drive\n",
        "from google.colab import drive\n",
        "save_path = \"path\"\n",
        "trainer.save_model(save_path)"
      ],
      "metadata": {
        "id": "s08SHxpI81tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the weights that you trained are saved on google drive with files like  adapter_config.jsonand training_args.bin"
      ],
      "metadata": {
        "id": "p1Kr4_jP81oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This below script conatins code to merge weights into the base model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Base model\n",
        "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# LoRA checkpoint in Google Drive\n",
        "lora_path = \"/content/drive/MyDrive/fine_tune_outputs/checkpoint-5000\"\n",
        "\n",
        "# Where to save merged model in Drive\n",
        "save_path = \"/content/drive/MyDrive/merged_tinyllama_model\"\n",
        "\n",
        "# Load base model + tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# Load LoRA and merge into base model\n",
        "model = PeftModel.from_pretrained(model, lora_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save merged model to Drive\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Merged model saved at {save_path}\")\n"
      ],
      "metadata": {
        "id": "0GIFoKKT81lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merged model saved on drive now should contains files like model.safetensors and tokenizer.json and tokenizer.model. This process usually takes 3 to 5 minutes"
      ],
      "metadata": {
        "id": "mTr-WIKK81iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BKOgvlno8w_t"
      },
      "outputs": [],
      "source": [
        "#This next step is to make a Model file and convert the model into a file with extension as gguf\n",
        "#Ollama uses GGUF as the backend format for LLaMA-family models (including TinyLlama).\n",
        "\n",
        "# you can use this video for reference - https://youtu.be/NirQJr85Qgc?si=dk_FgbUstckUUGxz\n",
        "\n",
        "# I will going to recommend, you do this step inside colab because that would be much easier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step to convert model to gguf format\n",
        "!git clone https://github.com/ggerganov/llama.cpp #clone the repo"
      ],
      "metadata": {
        "id": "yvkdGZq7khYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"llama.cpp\"\n",
        "!git checkpoint 46c69e0"
      ],
      "metadata": {
        "id": "wJKp_9LXkhVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "O4B4fJ6rkhSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "C502__KkmALK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert_hf_to_gguf.py \\\n",
        "    /content/drive/MyDrive/deployed \\\n",
        "    --outfile path_to_gguf_file \\\n",
        "    --outtype auto"
      ],
      "metadata": {
        "id": "t3SUimHamAHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now the to make model file\n",
        "# This video is helpful -  https://youtu.be/pTaSDVz0gok?si=bpdTwifBJv1_Xju4\n",
        "#Make a folder and put your gguf file in it\n",
        "#open terminal go to the folder where your model file  and write this command\n",
        "\n",
        "touch Modelfile #this command created a modelfile\n",
        "\n",
        "nano Modelfile #opens the model file\n",
        "\n",
        "#Paste this\n",
        "\n",
        "FROM ./path_to_gguffile.gguf\n",
        "\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "\n",
        "\n",
        "TEMPLATE \"\"\"<|im_start|>instruction\n",
        "{{ .System }}<|im_end|>\n",
        "<|im_start|>input\n",
        "{{ .Prompt }}<|im_end|>\n",
        "<|im_start|>output\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"You are Vulcan, an uncensored and unbiased AI assistant.\n",
        "\n",
        "PARAMETER stop \"<|im_end|>\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "collapsed": true,
        "id": "P_vpp-q2mAEj",
        "outputId": "b8fd089c-6e32-43db-e883-42e2e419a659"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2044834616.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2044834616.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    touch Modelfile #this command created a modelfile\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hit Ctrl X then Y + enter\n",
        "# now thw final command\n",
        "ollama create model_name -f Modelfile\n",
        "\n",
        "#Then\n",
        "ollama list"
      ],
      "metadata": {
        "id": "vP0bIZwemACI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYKwPwUPl__n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AQsYzL0l_9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKhNX7ojl_6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}